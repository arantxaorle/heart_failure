{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import dirname, abspath\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIPELINE_SWITCH': {'PIPELINE_TEST': False}, 'FEATURE_SET_VARS': {'IMPUTATION': 'Linear', 'MIN_THRESHOLD': 5}, 'PANDAS': {'WEIGHT': 80, 'HEIGHT': 180, 'CONTROL': False}, 'FOLDERS': {'DATA': 'data', 'RAW_FEATURESET': 'raw_featureset', 'TEMPORAL': 'temporal_data', 'POST_PROCESSED_FEATURESET': 'post_processed_featureset'}, 'FEATURE_SET': {'RAW_FEATURESET_EXCEL': 'Datos.xlsx', 'SPLIT_LABEL_NAME': 'HeartDisease', 'FEATURESET_EXCEL': 'Data_featureset.xlsx', 'LABELS_EXCEL': 'Data_labelset.xlsx', 'POSTPROCESSING_STEPS': {'DATA_IMPUTATION': True}}}\n"
     ]
    }
   ],
   "source": [
    "from config import config_variables\n",
    "print(config_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataFile(object):\n",
    "    def __init__(self, config_variables):\n",
    "        self.config = config_variables\n",
    "        self.base_path = (abspath(os.getcwd()))\n",
    "\n",
    "        self.data_folder = config_variables['FOLDERS']['DATA']\n",
    "        self.raw_data_folder = config_variables['FOLDERS']['RAW_FEATURESET']\n",
    "        self.raw_excel_file = config_variables['FEATURE_SET']['RAW_FEATURESET_EXCEL']\n",
    "        self.path_input_excel = os.path.join(self.base_path, self.data_folder, self.raw_data_folder, self.raw_excel_file)\n",
    "        self.raw_heart_df = pd.read_excel(self.path_input_excel)\n",
    "\n",
    "        self.temporal_data_folder = config_variables['FOLDERS']['TEMPORAL']\n",
    "        self.label_name = config_variables['FEATURE_SET']['SPLIT_LABEL_NAME']\n",
    "        self.target_name = config_variables['FEATURE_SET']['SPLIT_LABEL_NAME']\n",
    "\n",
    "        self.feature_file = config_variables['FEATURE_SET']['FEATURESET_EXCEL']\n",
    "        self.output_feature_file = os.path.join(self.base_path, self.data_folder, self.temporal_data_folder, self.feature_file)\n",
    "        self.features = pd.read_excel(self.output_feature_file)\n",
    "    \n",
    "  \n",
    "        self.label_file = config_variables['FEATURE_SET']['LABELS_EXCEL']\n",
    "        self.output_label_file = os.path.join(self.base_path, self.data_folder, self.temporal_data_folder, self.label_file)\n",
    "        \n",
    "\n",
    "        #self.post_data_folder = config_variables['FOLDERS']['POST_PROCESSED_FEATURESET']\n",
    "        #self.post_processed_featureset_excel_file = config_variables['FEATURE_SET']['POSTPROCESSED_FEATURESET_EXCEL']\n",
    "        #self.path_output_excel = os.path.join(self.base_path, self.data_folder, self.post_data_folder, self.post_processed_featureset_excel_file)\n",
    "        #self.feature_heart_df = pd.read_excel(self.output_feature_file)\n",
    "    \n",
    "    def split_labels_target(self, config_variables):\n",
    "         #Check if folder exists\n",
    "        if os.path.exists(os.path.join(self.base_path, self.data_folder, self.temporal_data_folder)) == False:\n",
    "            #Create folder\n",
    "            os.mkdir(os.path.join(self.base_path, self.data_folder, self.temporal_data_folder))\n",
    "        else:\n",
    "             print('Folder already exists')\n",
    "        \n",
    "        #Check if featureset file exists \n",
    "        if os.path.exists(os.path.join(self.base_path, self.data_folder, self.temporal_data_folder, self.output_feature_file)) == True:\n",
    "            print('Feature set file already exists')\n",
    "\n",
    "        #Check if label file exists \n",
    "        if os.path.exists(os.path.join(self.base_path, self.data_folder, self.temporal_data_folder, self.output_label_file)) == True:\n",
    "            print('Label set file already exists')\n",
    "\n",
    "        #Split labels\n",
    "        \n",
    "        self.feature_set = self.raw_heart_df.loc[:, self.raw_heart_df.columns != self.label_name]\n",
    "        self.feature_df = self.feature_set.to_excel(self.output_feature_file)\n",
    "        \n",
    "        #Split target \n",
    "        \n",
    "        self.label_set = self.raw_heart_df[self.label_name]\n",
    "        self.label_df = self.label_set.to_excel(self.output_label_file)\n",
    "\n",
    "\n",
    "    def data_imputation(self, config_variables):\n",
    "        #Read excel feature file\n",
    "        self.feature_excel = os.path.join(self.base_path, self.data_folder, self.temporal_data_folder, self.output_label_file)\n",
    "        self.feature_heart_df = pd.read_excel(self.feature_excel)\n",
    "\n",
    "        #Load input featureset\n",
    "        #inputdata_df = self.feature_heart_df\n",
    "\n",
    "        #Separate numeric variables \n",
    "        self.integer_features = self.feature_heart_df.select_dtypes(exclude=\"object\").columns\n",
    "        #print(self.integer_features)\n",
    "\n",
    "        #Separate categorical variables \n",
    "        self.categorical_features = self.feature_heart_df.select_dtypes(include=\"object\").columns\n",
    "        #print(self.categorical_features)\n",
    "\n",
    "        #Delete outliers\n",
    "        #self.feature_heart_df.boxplot(self.integer_features)\n",
    "\n",
    "        #Define num_pipeline\n",
    "        #Standarization \n",
    "        num_pipeline = Pipeline([\n",
    "               ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "        #Normalization\n",
    "        num_pipeline = Pipeline([\n",
    "               ('scaler', MinMaxScaler()),\n",
    "        ])\n",
    "\n",
    "        \n",
    "        #Apply One Hot Encoding\n",
    "        full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, self.integer_features),\n",
    "            (\"cat\", OneHotEncoder(), self.categorical_features),\n",
    "        ])\n",
    "\n",
    "        heart_prepared = full_pipeline.fit_transform(self.feature_heart_df)\n",
    "        #print(heart_prepared)\n",
    "\n",
    "        names = get_feature_names(full_pipeline)\n",
    "        #print(names)\n",
    "\n",
    "        #Delete first column \n",
    "        features = pd.DataFrame(heart_prepared,columns=names)\n",
    "        features_final  = features.iloc[: , 1:]\n",
    "        #print(features_final)\n",
    " \n",
    "    def build_featureset_definitive(self):\n",
    "        #Check if featureset exists\n",
    "        if os.path.exists(self.path_output_excel) == False:\n",
    "            #Check if folder exists\n",
    "            if os.path.exists(os.path.join(self.base_path, self.data_folder, self.post_data_folder)) == False:\n",
    "                #Create folder\n",
    "                os.mkdir(os.path.join(self.base_path, self.data_folder, self.post_data_folder))\n",
    "            \n",
    "            #Load input featureset\n",
    "            #inputdata_df = self.feature_heart_df\n",
    "            \n",
    "            #Preprocessing steps\n",
    "            #if self.config['FEATURE_SET']['POSTPROCESSING_STEPS']['DATA_IMPUTATION']:\n",
    "            #    inputdata_df = self.data_imputation(inputdata_df)\n",
    "\n",
    "            #Save postprocessed featureset \n",
    "            #inputdata_df.to_excel(self.path_output_excel)\n",
    "\n",
    "        else: \n",
    "            print('No need to build featureset, file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "heartdata = HeartDataFile(config_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m heartdata\u001b[39m.\u001b[39;49msplit_labels_target(config_variables)\n",
      "Cell \u001b[1;32mIn [9], line 39\u001b[0m, in \u001b[0;36mHeartDataFile.split_labels_target\u001b[1;34m(self, config_variables)\u001b[0m\n\u001b[0;32m     36\u001b[0m      \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFolder already exists\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[39m#Check if featureset file exists \u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_folder, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemporal_data_folder, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures)) \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFeature set file already exists\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[39m#Check if label file exists \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py:143\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m result_drive \u001b[39m+\u001b[39m result_path\n\u001b[0;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mBytesWarning\u001b[39;00m):\n\u001b[1;32m--> 143\u001b[0m     genericpath\u001b[39m.\u001b[39;49m_check_arg_types(\u001b[39m'\u001b[39;49m\u001b[39mjoin\u001b[39;49m\u001b[39m'\u001b[39;49m, path, \u001b[39m*\u001b[39;49mpaths)\n\u001b[0;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m         hasbytes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m() argument must be str, bytes, or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    153\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mos.PathLike object, not \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m hasstr \u001b[39mand\u001b[39;00m hasbytes:\n\u001b[0;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt mix strings and bytes in path components\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'DataFrame'"
     ]
    }
   ],
   "source": [
    "heartdata.split_labels_target(config_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR',\n",
      "       'Oldpeak'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arant\\AppData\\Local\\Temp\\ipykernel_12004\\1809191144.py:30: UserWarning: Transformer scaler (type MinMaxScaler) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "c:\\Users\\arant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#I should delete the first column \n",
    "heartdata.data_imputation(config_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing\n",
    "    def __init__(self, arg):\n",
    "        self.arg = arg (attributes necessary for the loading of data)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}'\n",
    "                f'(rank={self.rank!r}, suit={self.suit!r})')\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other.__class__ is not self.__class__:\n",
    "            return NotImplemented\n",
    "        return (self.rank, self.suit) == (other.rank, other.suit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "      def __init__(self, arg):\n",
    "          self.arg = arg (attributes necessary for the loading of data)\n",
    "          \n",
    "      def load_data(self):\n",
    "          \"load the data from somewhere and reshape it so that it can be used by method1\"\n",
    "          return(data)\n",
    "\n",
    "      def method1(self, data):\n",
    "          \"do some transformation on the data\"\n",
    "          return(data1)\n",
    "\n",
    "      def method2(self, data1):\n",
    "          \"do some transformation on the data1\"\n",
    "          return(data2)\n",
    "\n",
    "      def method3(self, data2):\n",
    "           \"do some transformation on the data2\"\n",
    "          return(data3)\n",
    "\n",
    "      def run(self):\n",
    "           data = self.load()\n",
    "           data1 = self.method1(data)\n",
    "           data2 = self.method2(data1)\n",
    "           data3 = self.method3(data2)\n",
    "           return(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "def call(self, inputs):\n",
    "    return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate target variable\n",
    "col = \"HeartDisease\"\n",
    "heart = data.loc[:, data.columns != col]\n",
    "heart.head()\n",
    "heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_heart = heart.select_dtypes(include=[np.number])\n",
    "categorical_heart = heart.select_dtypes(exclude=[np.number])\n",
    "\n",
    "numeric_heart.shape[1]\n",
    "categorical_heart.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "               ('std_scaler', StandardScaler()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_attribs = integer_features\n",
    "cat_attribs = categorical_features\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "777a9d555769d94f8b34f8532681f098915e1e0d5bac5997fe5ed84cae912a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
